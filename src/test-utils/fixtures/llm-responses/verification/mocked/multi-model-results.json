{
  "_metadata": {
    "capturedAt": "2026-01-10T12:00:00.000Z",
    "model": "multi-model",
    "scenario": "verification-with-multiple-models",
    "backendUrl": "http://localhost:5001",
    "isMocked": true,
    "description": "Verification results for 2 answering models (Claude Haiku and GPT-4) with 2 parsing models"
  },
  "data": {
    "q1_anthropic/claude-haiku-4-5_anthropic/claude-haiku-4-5_1768041830000": {
      "metadata": {
        "question_id": "q1",
        "template_id": "fcb63001d642141a76ea74a28088cc3a",
        "completed_without_errors": true,
        "error": null,
        "question_text": "What is the capital of France?",
        "raw_answer": "The capital of France is Paris.",
        "keywords": null,
        "answering_model": "anthropic/claude-haiku-4-5",
        "parsing_model": "anthropic/claude-haiku-4-5",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 1.523,
        "timestamp": "2026-01-10 12:00:00",
        "result_id": "mock-result-haiku-q1",
        "run_name": "multi-model-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "The capital of France is Paris.",
        "parsed_gt_response": { "capital": "Paris" },
        "parsed_llm_response": { "capital": "Paris" },
        "template_verification_performed": true,
        "verify_result": true,
        "verify_granular_result": { "capital_match": true },
        "abstention_check_performed": true,
        "abstention_detected": false
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    },
    "q1_openai/gpt-4_anthropic/claude-haiku-4-5_1768041830100": {
      "metadata": {
        "question_id": "q1",
        "template_id": "fcb63001d642141a76ea74a28088cc3a",
        "completed_without_errors": true,
        "error": null,
        "question_text": "What is the capital of France?",
        "raw_answer": "The capital of France is Paris.",
        "keywords": null,
        "answering_model": "openai/gpt-4",
        "parsing_model": "anthropic/claude-haiku-4-5",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 2.145,
        "timestamp": "2026-01-10 12:00:01",
        "result_id": "mock-result-gpt4-q1",
        "run_name": "multi-model-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "Paris is the capital of France.",
        "parsed_gt_response": { "capital": "Paris" },
        "parsed_llm_response": { "capital": "Paris" },
        "template_verification_performed": true,
        "verify_result": true,
        "verify_granular_result": { "capital_match": true },
        "abstention_check_performed": true,
        "abstention_detected": false
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    },
    "q2_anthropic/claude-haiku-4-5_anthropic/claude-haiku-4-5_1768041830200": {
      "metadata": {
        "question_id": "q2",
        "template_id": "9fb7f87b18c8c14e2203e010aa6d05de",
        "completed_without_errors": true,
        "error": null,
        "question_text": "Calculate 15 + 27.",
        "raw_answer": "42",
        "keywords": null,
        "answering_model": "anthropic/claude-haiku-4-5",
        "parsing_model": "anthropic/claude-haiku-4-5",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 0.987,
        "timestamp": "2026-01-10 12:00:02",
        "result_id": "mock-result-haiku-q2",
        "run_name": "multi-model-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "15 + 27 = 42",
        "parsed_gt_response": { "result": 42 },
        "parsed_llm_response": { "result": 42 },
        "template_verification_performed": true,
        "verify_result": true,
        "verify_granular_result": { "result_match": true },
        "abstention_check_performed": true,
        "abstention_detected": false
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    },
    "q2_openai/gpt-4_anthropic/claude-haiku-4-5_1768041830300": {
      "metadata": {
        "question_id": "q2",
        "template_id": "9fb7f87b18c8c14e2203e010aa6d05de",
        "completed_without_errors": true,
        "error": null,
        "question_text": "Calculate 15 + 27.",
        "raw_answer": "42",
        "keywords": null,
        "answering_model": "openai/gpt-4",
        "parsing_model": "anthropic/claude-haiku-4-5",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 1.234,
        "timestamp": "2026-01-10 12:00:03",
        "result_id": "mock-result-gpt4-q2",
        "run_name": "multi-model-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "The sum of 15 and 27 is 42.",
        "parsed_gt_response": { "result": 42 },
        "parsed_llm_response": { "result": 42 },
        "template_verification_performed": true,
        "verify_result": true,
        "verify_granular_result": { "result_match": true },
        "abstention_check_performed": true,
        "abstention_detected": false
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    },
    "q3_anthropic/claude-haiku-4-5_openai/gpt-4_1768041830400": {
      "metadata": {
        "question_id": "q3",
        "template_id": "0ece68c26113df796e88f7a1312bf830",
        "completed_without_errors": true,
        "error": null,
        "question_text": "Name three primary colors.",
        "raw_answer": "Red, blue, and yellow.",
        "keywords": null,
        "answering_model": "anthropic/claude-haiku-4-5",
        "parsing_model": "openai/gpt-4",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 1.876,
        "timestamp": "2026-01-10 12:00:04",
        "result_id": "mock-result-haiku-q3-gpt4-parse",
        "run_name": "multi-model-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "The three primary colors are red, blue, and yellow.",
        "parsed_gt_response": { "colors": ["red", "blue", "yellow"] },
        "parsed_llm_response": { "colors": ["red", "blue", "yellow"] },
        "template_verification_performed": true,
        "verify_result": true,
        "verify_granular_result": { "all_colors_present": true },
        "abstention_check_performed": true,
        "abstention_detected": false
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    },
    "q3_openai/gpt-4_openai/gpt-4_1768041830500": {
      "metadata": {
        "question_id": "q3",
        "template_id": "0ece68c26113df796e88f7a1312bf830",
        "completed_without_errors": true,
        "error": null,
        "question_text": "Name three primary colors.",
        "raw_answer": "Red, blue, and yellow.",
        "keywords": null,
        "answering_model": "openai/gpt-4",
        "parsing_model": "openai/gpt-4",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 2.567,
        "timestamp": "2026-01-10 12:00:05",
        "result_id": "mock-result-gpt4-q3",
        "run_name": "multi-model-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "Red, blue, and yellow are the three primary colors.",
        "parsed_gt_response": { "colors": ["red", "blue", "yellow"] },
        "parsed_llm_response": { "colors": ["red", "blue", "yellow"] },
        "template_verification_performed": true,
        "verify_result": true,
        "verify_granular_result": { "all_colors_present": true },
        "abstention_check_performed": true,
        "abstention_detected": false
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    }
  }
}
