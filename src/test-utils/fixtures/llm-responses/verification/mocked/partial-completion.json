{
  "_metadata": {
    "capturedAt": "2026-01-10T12:00:00.000Z",
    "model": "mock",
    "scenario": "partial-completion",
    "backendUrl": "http://localhost:5001",
    "isMocked": true
  },
  "data": {
    "q1_anthropic/claude-haiku-4-5_anthropic/claude-haiku-4-5_1768041830000": {
      "metadata": {
        "question_id": "q1",
        "template_id": "fcb63001d642141a76ea74a28088cc3a",
        "completed_without_errors": true,
        "error": null,
        "question_text": "What is the capital of France?",
        "raw_answer": "The capital of France is Paris.",
        "keywords": null,
        "answering_model": "anthropic/claude-haiku-4-5",
        "parsing_model": "anthropic/claude-haiku-4-5",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 1.523,
        "timestamp": "2026-01-10 12:00:00",
        "result_id": "mock-result-006",
        "run_name": "mock-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "The capital of France is Paris.",
        "parsed_gt_response": { "capital": "Paris" },
        "parsed_llm_response": { "capital": "Paris" },
        "template_verification_performed": true,
        "verify_result": true,
        "verify_granular_result": { "capital_match": true },
        "embedding_check_performed": false,
        "embedding_similarity_score": null,
        "embedding_override_applied": false,
        "embedding_model_used": null,
        "regex_validations_performed": false,
        "regex_validation_results": null,
        "regex_validation_details": null,
        "regex_overall_success": null,
        "regex_extraction_results": null,
        "recursion_limit_reached": false,
        "abstention_check_performed": true,
        "abstention_detected": false,
        "abstention_override_applied": false,
        "abstention_reasoning": null,
        "answering_mcp_servers": null,
        "usage_metadata": {
          "input_tokens": 45,
          "output_tokens": 32,
          "total_tokens": 77
        },
        "agent_metrics": null
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    },
    "q2_anthropic/claude-haiku-4-5_anthropic/claude-haiku-4-5_1768041830000": {
      "metadata": {
        "question_id": "q2",
        "template_id": "9fb7f87b18c8c14e2203e010aa6d05de",
        "completed_without_errors": false,
        "error": "Template validation failed: Invalid template syntax",
        "question_text": "Calculate 15 + 27.",
        "raw_answer": null,
        "keywords": null,
        "answering_model": "anthropic/claude-haiku-4-5",
        "parsing_model": "anthropic/claude-haiku-4-5",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 0.012,
        "timestamp": "2026-01-10 12:00:01",
        "result_id": "mock-result-007",
        "run_name": "mock-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "",
        "parsed_gt_response": null,
        "parsed_llm_response": null,
        "template_verification_performed": false,
        "verify_result": null,
        "verify_granular_result": null,
        "embedding_check_performed": false,
        "embedding_similarity_score": null,
        "embedding_override_applied": false,
        "embedding_model_used": null,
        "regex_validations_performed": false,
        "regex_validation_results": null,
        "regex_validation_details": null,
        "regex_overall_success": null,
        "regex_extraction_results": null,
        "recursion_limit_reached": false,
        "abstention_check_performed": false,
        "abstention_detected": null,
        "abstention_override_applied": false,
        "abstention_reasoning": null,
        "answering_mcp_servers": null,
        "usage_metadata": null,
        "agent_metrics": null
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    },
    "q3_anthropic/claude-haiku-4-5_anthropic/claude-haiku-4-5_1768041830000": {
      "metadata": {
        "question_id": "q3",
        "template_id": "0ece68c26113df796e88f7a1312bf830",
        "completed_without_errors": true,
        "error": null,
        "question_text": "Name three primary colors.",
        "raw_answer": "Red, blue, and yellow.",
        "keywords": null,
        "answering_model": "anthropic/claude-haiku-4-5",
        "parsing_model": "anthropic/claude-haiku-4-5",
        "answering_system_prompt": "You are a helpful assistant.",
        "parsing_system_prompt": "You are a helpful assistant.",
        "execution_time": 0.756,
        "timestamp": "2026-01-10 12:00:02",
        "result_id": "mock-result-008",
        "run_name": "mock-verification",
        "replicate": null
      },
      "template": {
        "raw_llm_response": "The three primary colors in traditional color theory are red, blue, and yellow.",
        "parsed_gt_response": { "colors": ["red", "blue", "yellow"] },
        "parsed_llm_response": { "colors": ["red", "blue", "yellow"] },
        "template_verification_performed": true,
        "verify_result": true,
        "verify_granular_result": { "all_colors_present": true },
        "embedding_check_performed": false,
        "embedding_similarity_score": null,
        "embedding_override_applied": false,
        "embedding_model_used": null,
        "regex_validations_performed": false,
        "regex_validation_results": null,
        "regex_validation_details": null,
        "regex_overall_success": null,
        "regex_extraction_results": null,
        "recursion_limit_reached": false,
        "abstention_check_performed": true,
        "abstention_detected": false,
        "abstention_override_applied": false,
        "abstention_reasoning": null,
        "answering_mcp_servers": null,
        "usage_metadata": {
          "input_tokens": 32,
          "output_tokens": 22,
          "total_tokens": 54
        },
        "agent_metrics": null
      },
      "rubric": null,
      "deep_judgment": null,
      "deep_judgment_rubric": null,
      "evaluation_input": null,
      "used_full_trace": true,
      "trace_extraction_error": null
    }
  }
}
